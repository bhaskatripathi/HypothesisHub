{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRXSWKCrWvo9XJqLaPv16v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaskatripathi/HypothesisHub/blob/main/Hypothesis_Hub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hypothesis Hub**: *An AI Tool for Automated Research Question and Hypothesis Generation from a given Scientific Literature*"
      ],
      "metadata": {
        "id": "CiJGS2kfWEhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m1H_uP4zi5M",
        "outputId": "60c68323-5940-477d-d0eb-4918b4c391ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: OpenAI in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.137)\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.25.0-py3-none-any.whl (17.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from OpenAI) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from OpenAI) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from OpenAI) (4.65.0)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.47)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.2.4)\n",
            "Collecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from gradio) (8.4.0)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.10-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0\n",
            "  Downloading websockets-11.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.2.0)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from gradio) (3.7.1)\n",
            "Collecting huggingface-hub>=0.13.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.2.2)\n",
            "Collecting gradio-client>=0.0.8\n",
            "  Downloading gradio_client-0.0.10-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from gradio) (4.5.0)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->OpenAI) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->OpenAI) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->OpenAI) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->OpenAI) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->OpenAI) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->OpenAI) (22.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.0.8->gradio) (23.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio-client>=0.0.8->gradio) (2023.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.13.0->gradio) (3.11.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->OpenAI) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->OpenAI) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->OpenAI) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (5.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn->gradio) (8.1.3)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.6.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=9b4dd13569fddb045a277a066d2dfdae71a02e5cea20f07cc01f01c749ee5496\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uc-micro-py, semantic-version, python-multipart, orjson, h11, aiofiles, uvicorn, starlette, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.1.0 fastapi-0.95.0 ffmpy-0.3.0 gradio-3.25.0 gradio-client-0.0.10 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.13.4 linkify-it-py-2.0.0 mdit-py-plugins-0.3.3 orjson-3.8.10 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.1 websockets-11.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install OpenAI langchain gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain, SequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import SimpleMemory\n",
        "import os"
      ],
      "metadata": {
        "id": "VwyIL17MVE0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The following code generates three research questions and hypothesis (Ho,H1) for each of the research questions.**\n",
        "The code can be modified to address more research questions based on individual needs"
      ],
      "metadata": {
        "id": "euCjPxxhVDZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sequence Diagram**"
      ],
      "metadata": {
        "id": "uDjBumR7Xf-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://raw.githubusercontent.com/bhaskatripathi/HypothesisHub/main/Sequence%20diagram.PNG')\n"
      ],
      "metadata": {
        "id": "xO5IJiUXVinn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "1aede437-2f9f-46b9-b358-094d0525a14d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/bhaskatripathi/HypothesisHub/main/Sequence%20diagram.PNG\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchAndHypothesisGenerator:\n",
        "    def __init__(self, openai_api_key):\n",
        "        self.llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key,model_name=\"text-davinci-003\")\n",
        "\n",
        "        research_question_template = \"\"\"Given the following text, generate three research questions related to the topic.\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Research Question 1: \n",
        "Research Question 2: \n",
        "Research Question 3: \"\"\"\n",
        "        self.rq_prompt_template = PromptTemplate(input_variables=[\"text\"], template=research_question_template)\n",
        "        self.research_question_chain = LLMChain(\n",
        "            llm=self.llm, prompt=self.rq_prompt_template, output_key=\"research_questions\"\n",
        "        )\n",
        "\n",
        "        hypothesis_template = \"\"\"Given the research question \"{research_question}\", generate a null hypothesis (H0) and an alternate hypothesis (H1).\n",
        "Null Hypothesis (H0):\n",
        "Alternate Hypothesis (H1):\"\"\"\n",
        "        self.hypothesis_prompt_template = PromptTemplate(input_variables=[\"research_question\"], template=hypothesis_template)\n",
        "        self.hypothesis_chain = LLMChain(\n",
        "            llm=self.llm, prompt=self.hypothesis_prompt_template, output_key=\"hypotheses\"\n",
        "        )\n",
        "\n",
        "    def generate_research_questions(self, user_text):\n",
        "        result = self.research_question_chain({\"text\": user_text})\n",
        "        research_questions = [rq.strip() for rq in result[\"research_questions\"].split('\\n') if rq.startswith('Research Question')]\n",
        "        return research_questions\n",
        "\n",
        "    def generate_hypotheses1(self, research_questions):\n",
        "        hypotheses = []\n",
        "        for question in research_questions:\n",
        "            result = self.hypothesis_chain({\"research_question\": question})\n",
        "            hypothesis_lines = [line.strip() for line in result[\"hypotheses\"].split('\\n') if line.strip()]\n",
        "            if len(hypothesis_lines) == 1:\n",
        "                hypothesis = hypothesis_lines[0]\n",
        "                if \"H1:\" in hypothesis:\n",
        "                    h1 = hypothesis.replace(\"H1:\", \"\").strip()\n",
        "                    h0 = f\"There are {h1.lower()} between the different {question.split(':')[1].strip()}.\"\n",
        "                else:\n",
        "                    h0 = hypothesis\n",
        "                    h1 = \"N/A\"\n",
        "            elif len(hypothesis_lines) == 2:\n",
        "                h0 = hypothesis_lines[0].replace(\"H0:\", \"\").strip()\n",
        "                h1 = hypothesis_lines[1].replace(\"H1:\", \"\").strip()\n",
        "            else:\n",
        "                print(f\"Error generating hypotheses for question: {question}\")\n",
        "                h0, h1 = \"N/A\", \"N/A\"\n",
        "            hypotheses.append((h0, h1))\n",
        "        return hypotheses\n",
        "\n",
        "    def generate_hypotheses(self, research_questions):\n",
        "      hypotheses = []\n",
        "      for question in research_questions:\n",
        "          result = self.hypothesis_chain({\"research_question\": question})\n",
        "          hypothesis_lines = [line.strip() for line in result[\"hypotheses\"].split('\\n') if line.strip()]\n",
        "\n",
        "          if len(hypothesis_lines) == 0:\n",
        "              hypotheses.append((\"N/A\", \"N/A\"))\n",
        "\n",
        "          elif len(hypothesis_lines) == 1:\n",
        "              # Generate H1 using LLMChain\n",
        "              h0 = hypothesis_lines[0]\n",
        "              h1_result = self.hypothesis_chain({\"research_question\": question, \"hypothesis_type\": \"alternate\"})\n",
        "              h1_lines = [line.strip() for line in h1_result[\"hypotheses\"].split('\\n') if line.strip()]\n",
        "              if len(h1_lines) == 1:\n",
        "                  h1 = h1_lines[0]\n",
        "              else:\n",
        "                  h1 = \"N/A\"\n",
        "              hypotheses.append((h0, h1))\n",
        "\n",
        "          else:\n",
        "              h0 = hypothesis_lines[0]\n",
        "              h1 = hypothesis_lines[1]\n",
        "              if h1 == \"N/A\":\n",
        "                  # Generate H1 using LLMChain\n",
        "                  h1_result = self.hypothesis_chain({\"research_question\": question, \"hypothesis_type\": \"alternate\"})\n",
        "                  h1_lines = [line.strip() for line in h1_result[\"hypotheses\"].split('\\n') if line.strip()]\n",
        "                  if len(h1_lines) == 1:\n",
        "                      h1 = h1_lines[0]\n",
        "                  else:\n",
        "                      h1 = \"N/A\"\n",
        "              hypotheses.append((h0, h1))\n",
        "\n",
        "      return hypotheses\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    user_text = \"\"\"Objective : The Effect of News on Stock Prices: Evidence from Natural Language Processing is a study aimed at analyzing the relationship between news and stock prices using natural\n",
        "     language processing techniques. The study analyzes news articles published by major financial news outlets and their impact on stock prices.\n",
        "      The researchers use a sentiment analysis model to analyze the tone of the news articles and determine whether they have a positive or negative impact on the stock prices. \n",
        "      The study finds that there is a significant relationship between news sentiment and stock prices, with positive news leading to an increase in stock prices and negative news leading \n",
        "      to a decrease in stock prices. The study highlights the importance of monitoring news and understanding its impact on the stock market.The study aims to investigate the effect of news \n",
        "      sentiment on stock prices using natural language processing techniques. The research focuses on analyzing news articles from various sources and measuring the sentiment of the news using \n",
        "      machine learning algorithms. The study aims to identify the relationship between news sentiment and stock prices and to explore whether news sentiment can be used as a predictor of stock prices. \n",
        "    The findings of the study can potentially have significant implications for investors and financial analysts in terms of identifying market trends and making informed investment decisions.\n",
        "    \"\"\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXXXX\" #\"you_openai_api_key\"\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        "    generator = ResearchAndHypothesisGenerator(api_key)\n",
        "    print(\"RUNNING CHAIN OF THOUGHTS...:\")\n",
        "    research_questions = generator.generate_research_questions(user_text)\n",
        "    print(\"ORIGINAL TEXT:\")\n",
        "    print(user_text)\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    print(\"LIST OF REQSEARCH QUESTIONS:\")\n",
        "    for i, question in enumerate(research_questions):\n",
        "        print(f\"{i + 1}: {question}\")\n",
        "\n",
        "    hypotheses = generator.generate_hypotheses(research_questions)\n",
        "    print(\"\\nLIST OF HYPOTHESIS:\")\n",
        "    for i, hypothesis_pair in enumerate(hypotheses):\n",
        "        print(f\"\\nRESEARCH QUESTION {i + 1}:\")\n",
        "        print(f\"NULL HYPOTHESIS (H0): {hypothesis_pair[0]}\")\n",
        "        print(f\"ALTERNATE HYPOTHESIS (H1): {hypothesis_pair[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsbXdyQ_5JYk",
        "outputId": "382de32e-de01-4a86-fd7d-222c75608647"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUNNING CHAIN OF THOUGHTS...:\n",
            "ORIGINAL TEXT:\n",
            "Objective : The Effect of News on Stock Prices: Evidence from Natural Language Processing is a study aimed at analyzing the relationship between news and stock prices using natural\n",
            "     language processing techniques. The study analyzes news articles published by major financial news outlets and their impact on stock prices.\n",
            "      The researchers use a sentiment analysis model to analyze the tone of the news articles and determine whether they have a positive or negative impact on the stock prices. \n",
            "      The study finds that there is a significant relationship between news sentiment and stock prices, with positive news leading to an increase in stock prices and negative news leading \n",
            "      to a decrease in stock prices. The study highlights the importance of monitoring news and understanding its impact on the stock market.The study aims to investigate the effect of news \n",
            "      sentiment on stock prices using natural language processing techniques. The research focuses on analyzing news articles from various sources and measuring the sentiment of the news using \n",
            "      machine learning algorithms. The study aims to identify the relationship between news sentiment and stock prices and to explore whether news sentiment can be used as a predictor of stock prices. \n",
            "    The findings of the study can potentially have significant implications for investors and financial analysts in terms of identifying market trends and making informed investment decisions.\n",
            "    \n",
            "\n",
            "\n",
            "LIST OF REQSEARCH QUESTIONS:\n",
            "1: Research Question 1: What is the impact of news sentiment on stock prices using natural language processing techniques?\n",
            "2: Research Question 2: What is the relationship between news articles from various sources and stock prices?\n",
            "3: Research Question 3: How can news sentiment be used to predict stock prices?\n",
            "\n",
            "LIST OF HYPOTHESIS:\n",
            "\n",
            "RESEARCH QUESTION 1:\n",
            "NULL HYPOTHESIS (H0): H0: There is no statistically significant impact of news sentiment on stock prices using natural language processing techniques.\n",
            "ALTERNATE HYPOTHESIS (H1): H1: There is a statistically significant impact of news sentiment on stock prices using natural language processing techniques.\n",
            "\n",
            "RESEARCH QUESTION 2:\n",
            "NULL HYPOTHESIS (H0): H0: There is no relationship between news articles from various sources and stock prices.\n",
            "ALTERNATE HYPOTHESIS (H1): H1: There is a relationship between news articles from various sources and stock prices.\n",
            "\n",
            "RESEARCH QUESTION 3:\n",
            "NULL HYPOTHESIS (H0): H0: News sentiment cannot be used to predict stock prices.\n",
            "ALTERNATE HYPOTHESIS (H1): H1: News sentiment can be used to predict stock prices.\n"
          ]
        }
      ]
    }
  ]
}